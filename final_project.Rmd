---
title: "Group 7"
author: "Festus Attah, Manisha Parajuli, Favour Onyido"
date: "2022-11-30"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
```


```{r}
sigmoid <- function(x){
  return(1/(1 + exp(- x)))
}
```

```{r}
set.seed(233)
n <- 1000
p <- 1
rx <- rnorm(n*p, 0, 1)
x <- matrix(rx,ncol=p)
beta <- rpois(p+1,3)
y <- as.vector(round(sigmoid(beta%*%t(cbind(1,x))+rnorm(n, 0, 1))))
```

```{r}
#cost function
cost <- function(beta, X, y){
  m <- length(y) #number of rows  of the training data
  h <- sigmoid(X %*% beta)
  J <- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m #log likelihood function
  return(J)
}
#gradient function: defines the slope of the LL function
grad <- function(beta, X, y){
  m <- length(y) 
  
  h <- sigmoid(X%*%beta)
  grad <- (t(X)%*%(h - y))/m
  return(grad)
}
Optim_log <- function(x, y) #minimizing the LL function using the optim function
{
  if(!is.matrix(x)){
    x = as.matrix(x)
  }
  m <- dim(x)[1]
  intercept <- rep(1, m)
  x = cbind(intercept, x)
  x_inverse <- solve(t(x)%*%x)
  beta <- x_inverse%*%t(x)%*%y
  costOpti <- optim(beta, fn = cost, gr = grad, X = x, y = y)
  
  return(costOpti$par)
}
```




```{r}
result <- Optim_log(x, y)
result
```



```{r}    
res1 <- glm(y~x,family = "binomial") #validating our function 
summary(res1)
```
## Bootsrap Confidence Interval

```{r}
bootstrap_confi <- function(x, y, b=20, alpha = 0.05){
  n <- dim(x)[1]
  p <- dim(x)[2]
  beta <- matrix(nrow = b, ncol = p+1)
  
  for (i in 1:b) {
    draw <- sample(1:n, n, replace = TRUE)
    boot_x <- x[draw,]
    boot_y <- y[draw]
    beta[i,] <- Optim_log(boot_x, boot_y)
  }
  
  beta_mean <- apply(beta, 2, mean) 
  beta_std_dev <- apply(beta, 2, sd) 
  lower_bound <- beta_mean - qnorm(1 - alpha/2)*beta_std_dev
  upper_bound <- beta_mean + qnorm(1 - alpha/2)*beta_std_dev
  confi_interval <- cbind(beta_mean,lower_bound, upper_bound)
  return(confi_interval)
}
```

```{r}
bootstrap_confi(x, y)
```
## Plotting the Predicted probabilities vrs. the true Data Points
```{r}
intercept <- rep(1, n)
px = cbind(intercept, x)
#data <- data.frame(y,intercept,x)
z <- px%*%result #predicted probabilities 
for (i in 1:dim(x)[2]){
temp <- x[,i]
otemp=temp[order(temp)]
oz=z[order(temp)]
plot(y ~ temp)
lines(sigmoid(oz)~otemp, lwd=2, col="green")  
}
     
```

```{r}

confusion_matrix <- function(x, y, cutoff = 0.5, prob=c(0.7,0.3)){

intercept <- rep(1, n)
px = cbind(intercept, x)

y <- as.matrix(y)
data <- cbind(y,px)
data_new <- data.frame(data)
colnames(data_new) <- cbind("dependent","independent")
set.seed(1)

sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob)
train <- data_new[sample, ]
test <- data_new[!sample, ]

model_predict <-  px%*%result


thres_pred <- ifelse(model_predict > cutoff, 1, 0)
p_class <- as.factor(thres_pred)
con_mat <- confusionMatrix(as.factor(data_new$dependent), p_class)
return(con_mat$table)
}

# confusion_matrix <- function(x, y, cutoff = 0.5, prob=c(0.7,0.3)){
# 
# intercept <- rep(1, n)
# px = cbind(intercept, x)
# 
# y <- as.matrix(y)
# data <- cbind(y,x)
# data_new <- data.frame(data)
# colnames(data_new) <- cbind("dependent","independent")
# 
# set.seed(1)
# sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
# train <- data_new[sample, ]
# test <- data_new[!sample, ]
# 
# intercept <- rep(1, n)
# px = cbind(intercept, data_new$independent)
# 
# model_predict <-  px%*%result
# 
# cutoff = 0.5
# thres_pred <- ifelse(model_predict > cutoff, 1, 0)
# p_class <- as.factor(thres_pred)
# con_mat <- confusionMatrix(as.factor(test$dependent), p_class)
# return(con_mat)
# }

#  y <- as.matrix(y)
#  data <- cbind(y,x)
#  data_new <- data.frame(data)
#  colnames(data_new) <- cbind("dependent","independent")
# 
# #splitting the data into training and test data
#  set.seed(1)
#  sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
#  train <- data_new[sample, ]
#  test <- data_new[!sample, ]
# 
#  train <- data_new[1:700,]
#  test <- data_new[701:1000,] #this not part of code
# 
#  model <- glm(dependent~independent,family = "binomial", data=train) #validating our function
#  summary(model)
# 
#  predicted <- predict(model, test, type="response")
# 
# # If p exceeds threshold of 0.5, 1 else 0
#  cutoff <- 0.5
#  thres_pred <- ifelse(predicted > cutoff, 1, 0)
# 
# # Convert to factor: p_class
# p_class <- as.factor(thres_pred)
# p_class <- factor(thres_pred, levels = levels(test[["th"]]))
# 
# 
# # Create confusion matrix
# con_mat <- confusionMatrix(as.factor(test$dependent), p_class)
```

```{r}
#Calculations from the confusion matrix

cc <- confusion_matrix(x,y)

cal_confusion_matrix <- function(mat_tab, cal="acc"){
  TN <- mat_tab[1]
FN <- mat_tab[2]
FP <- mat_tab[3]
TP <- mat_tab[4]

#Prevelance pre
pre <- (FN+TP)/(TP+TN+FP+FN)

#Accuracy acc
acc <- (TP+TN)/(TP+TN+FP+FN)

#Sensitivity sen
sen <- TP/(TP+FN)

#Specificity spe
spe <- TN/(TN +FP)

#False discovery ratio fdr
fdr <- FP/(FP+TP)

#Diagnostic odds ratio dor
#getting lr+
fpr <- 1-spe
lrp <- sen/fpr

#getting lr+
fnr <- 1-sen
lrn <- fnr/spe

#getter dor
dor <- lrp/lrn

 switch(cal,
        "pre"= return(paste("Prevelence is", pre)),
        "acc"= return(paste("Accuracy is", acc)),
        "sen"= return(paste("Sensitivity is",sen)),
        "spe"= return(paste("Specificity is",spe)),
        "fdr"= return(paste("False discovery ratio is",fdr)),
        "dor"= return(paste("Diagnostic odds ratio is",dor)))

}
  

```

```{r}
#Calculates results for cutoff between 0.1-0.9
cal2 <- function(x,y,cal){
  cutoff <- seq(0.1, 0.9, 0.1)
  nums <- rep(NA,9)
  for (i in 1:9){
    mat_tab <- confusion_matrix(x,y,cutoff[i])
    calculation <- cal_confusion_matrix(mat_tab,cal)
    nums[i] <- as.numeric(tail(strsplit(calculation,split=" ")[[1]],1))
  }
  
  plot(nums,cutoff,ylab="Cutoffs between 0.1-0.9", xlab="Calculated figures")
}

```

```{r}
cal2(x,y,"fdr")
```

